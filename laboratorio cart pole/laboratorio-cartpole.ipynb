{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6520c4",
   "metadata": {},
   "source": [
    "# Integrantes del Grupo:\n",
    "### José Luis Flores Tito\n",
    "### Ariel Huarachi Clemente\n",
    "### Shariel Aylin Verduguez Choquevillca\n",
    "### Duran Chambi Benjamin Ricardo Duran\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b2b05",
   "metadata": {},
   "source": [
    "## CartPole con Gymnasiun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0bd0b-749a-4ddc-8238-2e938f12f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Implementación de Q-Learning para el entorno CartPole-v1\n",
    "# ================================================\n",
    "\n",
    "# Importamos las librerías necesarias\n",
    "import gymnasium as gym          # Entornos de simulación (CartPole, MountainCar, etc.)\n",
    "import numpy as np               # Cálculos numéricos y manejo de arrays\n",
    "import matplotlib.pyplot as plt  # Gráficas de progreso\n",
    "import pickle                    # Guardar/cargar el modelo entrenado (tabla Q)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Función principal: entrena o ejecuta el agente\n",
    "# ------------------------------------------------\n",
    "def run(is_training=True, render=False):\n",
    "\n",
    "    # Crear el entorno de CartPole\n",
    "    # render_mode='human' muestra la simulación si render=True\n",
    "    env = gym.make('CartPole-v1', render_mode='human' if render else None)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Discretización del espacio de estados\n",
    "    # ------------------------------------------------\n",
    "    # El entorno CartPole tiene 4 variables continuas:\n",
    "    # 1. posición del carro\n",
    "    # 2. velocidad del carro\n",
    "    # 3. ángulo de la vara\n",
    "    # 4. velocidad angular de la vara\n",
    "    #\n",
    "    # Aquí se dividen esos rangos en 10 segmentos para discretizarlos.\n",
    "    pos_space = np.linspace(-2.4, 2.4, 10)\n",
    "    vel_space = np.linspace(-4, 4, 10)\n",
    "    ang_space = np.linspace(-.2095, .2095, 10)\n",
    "    ang_vel_space = np.linspace(-4, 4, 10)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Inicialización o carga de la tabla Q\n",
    "    # ------------------------------------------------\n",
    "    if is_training:\n",
    "        # Si estamos entrenando, inicializamos la tabla Q con ceros\n",
    "        # La tabla tiene dimensiones [pos][vel][ang][ang_vel][acción]\n",
    "        # 11x11x11x11x2 porque cada espacio tiene 10 divisiones + 1, y hay 2 acciones (izquierda/derecha)\n",
    "        q = np.zeros((len(pos_space)+1,\n",
    "                      len(vel_space)+1,\n",
    "                      len(ang_space)+1,\n",
    "                      len(ang_vel_space)+1,\n",
    "                      env.action_space.n))\n",
    "    else:\n",
    "        # Si no estamos entrenando, cargamos la tabla Q desde un archivo previamente guardado\n",
    "        f = open('cartpole.pkl', 'rb')\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Hiperparámetros del algoritmo Q-Learning\n",
    "    # ------------------------------------------------\n",
    "    learning_rate_a = 0.3          # α (alpha): tasa de aprendizaje\n",
    "    discount_factor_g = 0.99       # γ (gamma): factor de descuento\n",
    "    epsilon = 1                    # Probabilidad inicial de elegir una acción aleatoria (exploración)\n",
    "    epsilon_decay_rate = 0.00001   # Tasa de decrecimiento de epsilon\n",
    "    rng = np.random.default_rng()  # Generador de números aleatorios\n",
    "\n",
    "    # Lista para guardar las recompensas por episodio\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # Contador de episodios\n",
    "    i = 0\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Bucle principal de episodios\n",
    "    # ------------------------------------------------\n",
    "    while(True):\n",
    "\n",
    "        # Reiniciamos el entorno y obtenemos el estado inicial\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # Convertimos cada variable continua en un índice discreto (bin)\n",
    "        state_p = np.digitize(state[0], pos_space)   # posición\n",
    "        state_v = np.digitize(state[1], vel_space)   # velocidad\n",
    "        state_a = np.digitize(state[2], ang_space)   # ángulo\n",
    "        state_av = np.digitize(state[3], ang_vel_space)  # velocidad angular\n",
    "\n",
    "        # Variable para indicar si el episodio terminó\n",
    "        terminated = False\n",
    "\n",
    "        # Recompensa acumulada en este episodio\n",
    "        rewards = 0\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # Bucle interno: pasos dentro de un episodio\n",
    "        # ------------------------------------------------\n",
    "        while(not terminated and rewards < 10000):\n",
    "\n",
    "            # Política ε-greedy:\n",
    "            # Con probabilidad epsilon elige acción aleatoria (explorar)\n",
    "            # Si no, elige la mejor acción conocida (explotar)\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()  # acción aleatoria\n",
    "            else:\n",
    "                action = np.argmax(q[state_p, state_v, state_a, state_av, :])  # mejor acción conocida\n",
    "\n",
    "            # Ejecutamos la acción en el entorno\n",
    "            new_state, reward, terminated, _, _ = env.step(action)\n",
    "\n",
    "            # Discretizamos el nuevo estado\n",
    "            new_state_p = np.digitize(new_state[0], pos_space)\n",
    "            new_state_v = np.digitize(new_state[1], vel_space)\n",
    "            new_state_a = np.digitize(new_state[2], ang_space)\n",
    "            new_state_av = np.digitize(new_state[3], ang_vel_space)\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Actualización de la tabla Q (solo si estamos entrenando)\n",
    "            # ------------------------------------------------\n",
    "            if is_training:\n",
    "                q[state_p, state_v, state_a, state_av, action] += learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state_p, new_state_v, new_state_a, new_state_av, :])\n",
    "                    - q[state_p, state_v, state_a, state_av, action]\n",
    "                )\n",
    "\n",
    "            # Actualizamos el estado actual con el nuevo\n",
    "            state = new_state\n",
    "            state_p = new_state_p\n",
    "            state_v = new_state_v\n",
    "            state_a = new_state_a\n",
    "            state_av = new_state_av\n",
    "\n",
    "            # Acumulamos la recompensa\n",
    "            rewards += reward\n",
    "\n",
    "            # Si estamos solo ejecutando (no entrenando), mostramos progreso\n",
    "            if not is_training and rewards % 100 == 0:\n",
    "                print(f'Episodio: {i}  Recompensas: {rewards}')\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # Fin del episodio\n",
    "        # ------------------------------------------------\n",
    "        # Guardamos la recompensa total obtenida\n",
    "        rewards_per_episode.append(rewards)\n",
    "\n",
    "        # Calculamos el promedio de recompensas de los últimos 100 episodios\n",
    "        mean_rewards = np.mean(rewards_per_episode[max(0, len(rewards_per_episode)-100):])\n",
    "\n",
    "        # Imprimimos el progreso durante el entrenamiento\n",
    "        if is_training and i % 100 == 0:\n",
    "            print(f'Episodio: {i}  Recompensa: {rewards}  Epsilon: {epsilon:0.2f}  Recompensa media: {mean_rewards:0.1f}')\n",
    "\n",
    "        # Si el agente ya logra mantener el palo más de 1000 pasos de media → se detiene el entrenamiento\n",
    "        if mean_rewards > 1000:\n",
    "            break\n",
    "\n",
    "        # Disminuimos epsilon (menos exploración, más explotación)\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        # Aumentamos el contador de episodios\n",
    "        i += 1\n",
    "\n",
    "    # Cerramos el entorno al finalizar\n",
    "    env.close()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Guardar la tabla Q entrenada\n",
    "    # ------------------------------------------------\n",
    "    if is_training:\n",
    "        f = open('cartpole.pkl', 'wb')\n",
    "        pickle.dump(q, f)\n",
    "        f.close()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Generar gráfica de progreso\n",
    "    # ------------------------------------------------\n",
    "    mean_rewards = []\n",
    "    for t in range(i):\n",
    "        # Calcula la media móvil de las últimas 100 recompensas\n",
    "        mean_rewards.append(np.mean(rewards_per_episode[max(0, t-100):(t+1)]))\n",
    "\n",
    "    # Crear el gráfico\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.xlabel('Episodios')  \n",
    "    plt.ylabel('Recompensas Medias (Últimos 100 Episodios)')  \n",
    "    plt.title('Progreso del Entrenamiento')  \n",
    "    plt.savefig('cartpole.png')  # Guarda la imagen con los resultados\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Ejecución del programa\n",
    "# ------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Entrenamiento del agente\n",
    "    run(is_training=True, render=False)\n",
    "\n",
    "    # Ejecución del agente ya entrenado (descomentar para ver)\n",
    "    # run(is_training=False, render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1116981",
   "metadata": {},
   "source": [
    "## Ejecutamos el agente entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942945f-5eef-457a-8217-11e3373102a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #run(is_training=True, render=False)\n",
    "\n",
    "    run(is_training=False, render=True) # Ejecución del agente ya entrenado (descomentar para ver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch - myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
